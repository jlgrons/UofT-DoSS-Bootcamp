---
title: "Exercise 6: Statistical inference (III)"
author: "Siyue Yang"
date: "06/01/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Part 1: Wald, Score, and likelihood ratio test statistics

Write out the likelihood function, and derive the test statistics of the Wald, Score, and likelihood ratio test. 

1. $X_{i} \stackrel{\text { i.i.d. }}{\sim} f(x \mid \theta)$ 
 
$$f(x \mid \theta)=\theta \exp (-x \theta) \mathbb{I}\{x>0\}$$ 

2. $X_{i} \stackrel{\text { i.i.d. }}{\sim} f(x \mid \theta)$ 
$$
f(x \mid \theta)=\theta c^{\theta} x^{-(\theta+1)} \mathbb{I}\{x>c\} \quad \text { (Pareto distribution) }
$$
where $c$ is a known constant and $\theta$ is unknown.


**Solution**

1. 
The log-likelihood function is 
$$
l(\theta)=n\left(\log \theta-\theta \bar{X}_{n}\right)
$$
which yields
$$
l^{\prime}(\theta)=n\left(\frac{1}{\theta}-\bar{X}_{n}\right) \text { and } l^{\prime \prime}(\theta)=-\frac{n}{\theta^{2}}
$$
The MLE $\hat{\theta}_n$ is obtained by setting $l'(\theta)=0$,
$$
\widehat{\theta}_{n}=\frac{1}{\bar{X}_{n}}
$$
and the Fisher information can be obtained by 
$$
I(\theta)= \theta^{-2}
$$
It follows that,
$$
\begin{aligned}
W_{n} &=\frac{\sqrt{n}}{\theta_{0}}\left(\frac{1}{\bar{X}_{n}}-\theta_{0}\right) \\
R_{n} &=\theta_{0} \sqrt{n}\left(\frac{1}{\theta_{0}}-\bar{X}_{n}\right)=\frac{W_{n}}{\theta_{0} \bar{X}_{n}} \\
\Delta_{n} &=n\left\{\bar{X}_{n}\left(\bar{X}_{n}-\theta_{0}\right)-\log \left(\theta_{0} \bar{X}_{n}\right)\right\}
\end{aligned}
$$

2. If let $S_{n}=\sum_{i=1}^{n} \log \left(x_{i}\right)$, the log-likelihood function is 
$$
l(\theta) =n(\log \theta+\theta \log c)-(\theta+1) S_{n} 
$$
which yields,
$$
l^{\prime}(\theta) =n\left(\frac{1}{\theta}+\log c\right)-S_{n}, \quad l^{\prime \prime}(\theta)=-\frac{n}{\theta^{2}}
$$
The MLE can be obtained as 
$$
\widehat{\theta}_{n}=\frac{n}{S_{n}-n \log c}
$$
and Fisher information as 
$$
I(\theta)=\theta^{-2}
$$
Thus, the three test statistics are 
$$
\begin{aligned}
W_{n} &=\frac{\sqrt{n}}{\theta_{0}}\left(\frac{n}{S_{n}-n \log c}-\theta_{0}\right) \\
R_{n} &=\sqrt{n} \theta_{0}\left(\left(\frac{1}{\theta_{0}}+\log c\right)-S_{n}\right) \\
\Delta_{n} &=n\left(\log \frac{\widehat{\theta}_{n}}{\theta_{0}}+\left(\widehat{\theta}_{n}-\theta_{0}\right) \log c\right)-\left(\widehat{\theta}_{n}-\theta_{0}\right) S_{n}
\end{aligned}
$$


# Part 2: Test equivalence

Let $\theta$ be a scalar parameter and suppose we test
$$
H_{0}: \theta=\theta_{0} \quad \text { versus } \quad H_{1}: \theta \neq \theta_{0} .
$$
Let $W$ be the Wald test statistic and let $\lambda$ be the likelihood ratio test statistic. Show that these tests are equivalent in the sense that
$$
\frac{W^{2}}{\lambda} \stackrel{\mathrm{P}}{\longrightarrow} 1
$$
as $n \rightarrow \infty$. Hint: Use a Taylor expansion of the log-likelihood $\ell(\theta)$ to show that
$$
\lambda \approx\left(\sqrt{n}\left(\widehat{\theta}-\theta_{0}\right)\right)^{2}\left(-\frac{1}{n} \ell^{\prime \prime}(\widehat{\theta})\right)
$$

**Solution**

Throughout this proof, it is assumed that the density $f(x ; \theta)$ appearing in the likelihood is sufficiently regular. A Taylor expansion reveals
$$
\ell\left(\theta_{0}\right)=\ell(\hat{\theta})+\left(\hat{\theta}-\theta_{0}\right) \ell^{\prime}(\hat{\theta})+\frac{1}{2}\left(\hat{\theta}-\theta_{0}\right)^{2} \ell^{\prime \prime}(\hat{\theta})+O\left(\left(\hat{\theta}-\theta_{0}\right)^{3}\right) .
$$
Note, in particular, that $\ell^{\prime}(\hat{\theta})=0$ since $\hat{\theta}$ is an MLE. Therefore,
$$
\lambda=2 \log \left(\frac{\mathcal{L}(\hat{\theta})}{\mathcal{L}\left(\theta_{0}\right)}\right)=-\left(\hat{\theta}-\theta_{0}\right)^{2} \ell^{\prime \prime}(\hat{\theta})+O\left(\left(\hat{\theta}-\theta_{0}\right)^{3}\right)
$$
Moreover,
$$
W^{2}=\frac{\left(\hat{\theta}-\theta_{0}\right)^{2}}{\widehat{\operatorname{se}}(\hat{\theta})^{2}}=n I(\hat{\theta})\left(\hat{\theta}-\theta_{0}\right)^{2}
$$
It follows that
$$
\frac{\lambda}{W^{2}}=\frac{n^{-1} \ell^{\prime \prime}(\hat{\theta})}{-I(\hat{\theta})}+O\left(\hat{\theta}-\theta_{0}\right)
$$

Under the null hypothesis, $\hat{\theta} \stackrel{P}{\rightarrow} \theta_{0}$. Therefore, by two applications of Slutsky theorem, $1 / I(\hat{\theta}) \rightarrow 1 / I\left(\theta_{0}\right)$ where
$$
I\left(\theta_{0}\right)=\mathbb{E}_{\theta_{0}}\left[\frac{\partial^{2} \log f\left(X ; \theta_{0}\right)}{\partial \theta^{2}}\right]
$$
Since
$$
\ell^{\prime \prime}(\theta)=\sum_{n} \frac{\partial^{2} \log f\left(X_{n} ; \theta\right)}{\partial \theta^{2}}
$$
by the weak law of large numbers, $n^{-1} \ell^{\prime \prime}(\hat{\theta}) \stackrel{P}{\rightarrow} I\left(\theta_{0}\right)$ under the null hypothesis. The result now follows by Slutsky theorem. 


# Part 3: Omics
**The p-value is uniformly distributed when the null hypothesis is true.**

Let $T$ denote the random variable with cumulative distribution function $F(t) \equiv \operatorname{Pr}(T<t)$ for all $t$. Assuming that $F$ is invertible we can derive distribution of the random p-value $P=F(T)$ as follows:
$$
\operatorname{Pr}(P<p)=\operatorname{Pr}(F(T)<p)=\operatorname{Pr}\left(T<F^{-1}(p)\right)=F\left(F^{-1}(p)\right)=p,
$$
from which we can conclude that the distribution of $P$ is uniform on $[0,1]$.