\documentclass [aspectratio=169]{beamer}
\usetheme{Boadilla}
\usepackage{textpos} % package for the positioning
\usepackage[]{graphicx}
\usepackage{graphicx}
\usepackage{float}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algorithm,algpseudocode}
\usepackage{amsmath, amsfonts, amsthm, amssymb}
\usepackage{dsfont}
\usepackage[export]{adjustbox}
\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{arrows, shapes, decorations, automata, backgrounds, fit, petri, calc}
\setbeamertemplate{itemize items}[circle]
\setbeamertemplate{enumerate items}[circle]

\newcommand*{\logofont}{\fontfamily{phv}\selectfont}
\definecolor{uoftblue}{RGB}{0,42,92} % official blue color for uoft
\definecolor{deptgreen}{RGB}{114,192,148} 
\definecolor{deptoran}{RGB}{252,103,63} 

\hypersetup{
  colorlinks   = true, %Colours links instead of boxes
  urlcolor     = uoftblue, %Colour for external hyperlinks
  linkcolor    = black, %Colour of internal links
  citecolor   = black %Colour of citations
}

% lin alg
\newcommand{\bu}{{\mathbf{u}}}
\newcommand{\bv}{{\mathbf{v}}}
\newcommand{\bw}{{\mathbf{w}}}
\newcommand{\bx}{{\mathbf{x}}}
\newcommand{\by}{{\mathbf{y}}}
\newcommand{\bz}{{\mathbf{z}}}
\newcommand{\zerovec}{{\mathbf{0}}}
\newcommand{\innerprod}[1]{\langle #1 \rangle}
\newcommand{\Tr}{\mathrm{Tr}}
% other useful stuff
\newcommand{\Id}{{\mathds{1}}}
\newcommand{\R}{{\mathbb{R}}}
\newcommand{\C}{{\mathbb{C}}}
\newcommand{\Z}{{\mathbb{Z}}}
\newcommand{\N}{{\mathbb{N}}}
\newcommand{\Q}{{\mathbb{Q}}}
\newcommand{\F}{{\mathbb{F}}}
\newcommand{\cL}{{\mathcal{L}}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\inv}{{-1}}

\DeclareMathOperator{\range}{range}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\nullspace}{null}
\DeclareMathOperator{\nullity}{nullity}


\beamertemplatenavigationsymbolsempty

% block
% example block
% alert block

\setbeamercolor{coloredboxstuff}{fg=yellow,bg=uoftblue!10!white}
\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
  \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{coloredboxstuff}
    \usebeamerfont{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}



\title[]{Module 8: Linear Algebra II \\ {\large Operational math bootcamp}\\ \includegraphics[width=8cm]{dept_logo.png}\vspace{-1em}}
\author[]{Emma Kroell}
\institute[]{University of Toronto}
\date{July 22, 2022}

% set color
\setbeamercolor{title in head/foot}{bg=white}
\setbeamercolor{author in head/foot}{bg=white}
\setbeamercolor{date in head/foot}{fg=uoftblue}
\setbeamercolor{date in head/foot}{bg=white}
\setbeamercolor{title}{fg=uoftblue}
\setbeamerfont{title}{series=\bfseries}
\setbeamercolor{frametitle}{fg=uoftblue}
\setbeamerfont{frametitle}{series=\bfseries}
\setbeamercolor*{item}{fg=uoftblue}
\setbeamercolor{block title}{bg=uoftblue}
\setbeamercolor{block title}{fg=white}
\setbeamercolor{block body}{bg=uoftblue!9!white}
\setbeamercolor{block title example}{bg=deptgreen}
\setbeamercolor{block title example}{fg=white}
\setbeamercolor{block body example}{bg=deptgreen!13!white}
\setbeamercolor{block title alerted}{bg=deptoran}
\setbeamercolor{block title alerted}{fg=white}
\setbeamercolor{block body alerted}{bg=deptoran!10!white}


% set logo at non-title pages
\logo{\includegraphics[height=0.8cm]{dept_logo.png}\vspace*{-.045\paperheight}\hspace*{.78\paperwidth}}

% set margin
\setbeamersize{text margin left=10mm,text margin right=10mm}

\begin{document}
{
\setbeamertemplate{logo}{}
\begin{frame}
    %\vspace{0.5in}
    \titlepage
    %\begin{textblock*}{10cm}(3.5cm,-7.5cm)
      %  \includegraphics[width=8cm]{dept_logo.png}
    %\end{textblock*}
\end{frame}
}

\begin{frame}{Outline}
Last time:
    \begin{itemize}
      \setlength\itemsep{0.5em}
    	\item Linear independence and bases
	\item Linear maps, null space, range, inverses
	\item Matrices as linear maps
    \end{itemize}
    
\vspace{1.5em}

Today:
    \begin{itemize}
      \setlength\itemsep{0.5em}
    	\item Determinants
	\item Inner product spaces
    \end{itemize}
\end{frame}

\section{Determinants}

\begin{frame}{Determinant}
    \begin{itemize}
      \setlength\itemsep{0.5em}
    	\item The determinant is a function from $M_{n \times n} \to \F$, i.e. it is a function from the entries of a square matrix to a real or complex number.
	\item The determinant has applications in solving linear systems, computing eigenvalues, etc
    \end{itemize}

\end{frame}


\begin{frame}{Example: $2\times 2$ matrix}
The determinant of a $2 \times 2$ matrix is 
$$\left| \begin{matrix} a & b \\ c & d \end{matrix} \right| = $$
\end{frame}


\begin{frame}{Example: $3\times 3$ matrix}
There is a \textbf{\textcolor{deptoran}{trick}} for finding the determinant of a 3 by 3 matrix:
$$\left| \begin{matrix} a & b &c \\ d & e & f \\ g & h &i \end{matrix} \right| \qquad \qquad  \qquad \qquad \qquad \qquad \qquad \qquad$$
\vspace{1cm}
\end{frame}

\begin{frame}{Cofactor expansion}
For other $n \times n$ matrices, one can compute the determinant using \textbf{\textcolor{deptoran}{cofactor expansion}}.

\begin{definition}[Cofactor expansion]
Let $A = \{ a_{j,k}\}_{j,k=1}^n$ be a $n\times n$ matrix. Let $M_{j,k}$ denote the determinant of the $(n-1) \times (n-1)$ matrix obtained by removing the $j^\text{th}$ row and the $k^\text{th}$ column of $A$. For each row $j=1,\ldots,n$
\begin{equation*}
    |A| = \sum_{k=1}^n a_{j,k} (-1)^{j+k} M_{j,k}.
\end{equation*}
Similarly, for each column $k=1,\ldots,n$
\begin{equation*}
    |A| = \sum_{j=1}^n a_{j,k} (-1)^{j+k} M_{j,k}.
\end{equation*}
The numbers $C_{j,k}=(-1)^{j+k} M_{j,k}$ are called \emph{cofactors}.
\end{definition}
\end{frame}


\begin{frame}
\begin{exampleblock}{Proposition}
The determinant of a diagonal matrix or triangular matrix is the product of the entries on the diagonal.
\end{exampleblock}
\begin{block}{Sketch of proof}
\vspace{4cm}
\end{block}

\end{frame}


\begin{frame}{Inverse of a matrix}
\begin{theorem}
Let $A$ be an $n \times n$ invertible matrix and let $C=\{C_{j,k}\}_{j,k=1}^n$ be its cofactor matrix. Then
\begin{equation*}
    A^\inv = \frac{1}{|A|} C^T
\end{equation*}
\end{theorem}

\vspace{1em}

Connection to last lecture: The matrix $A$ is invertible if and only if the linear map represented by the matrix is an isomorphism.
\end{frame}

\begin{frame}{Cramer's rule}
\begin{corollary}
Suppose $A$ is an $n \times n$ invertible matrix. The linear system $A\bx = \mathbf{b}$ has a unique solution given by
\begin{equation*}
    x_i = \frac{|A_i|}{|A|}, \quad i, \ldots, n,
\end{equation*}
where $A_i$ is the matrix obtained by replacing the $i^\text{th}$ column of $A$ with $\mathbf{b}$.
\end{corollary}
\end{frame}

\begin{frame}{Transpose of a matrix}
\begin{definition}
The \emph{transpose} of an $m \times n$ matrix A is the $n \times m$ matrix, denoted $A^T$, defined entry-wise as $\{A^T_{j,k}\} = \{A_{k,j}\}$ for $j=1,\ldots,m$ and $k=1,\ldots n$ (i.e. the rows of $A$ are the columns of $A^T$ and the columns of $A$ are the rows of $A^T$)
\end{definition}
\vspace{1cm}
\end{frame}



\begin{frame}{Properties of the determinant}
\begin{exampleblock}{Proposition}
$|A| \neq 0$ if and only if $A$ is invertible
\end{exampleblock}

\begin{exampleblock}{Proposition}
Let $A$ be an $n \times n$ real matrix.
\begin{enumerate}
    \item If A has a zero column, then $|A| = 0$.
\item If A has two equal columns, then $|A| = 0$.
\item If one column of A is a multiple of another, then $|A| = 0$.
\item $|AB| = |A| |B|$
\item $|\alpha A| = \alpha^n |A|$ for $\alpha \in \F$
\item $|A^T| = |A|$
\end{enumerate}
\end{exampleblock}
\end{frame}

\section{Inner product spaces}


\begin{frame}{Complex numbers}
Recall that for a complex number $z = a + ib$, we define the following:
\vspace{0.5em}
\begin{itemize}
      \setlength\itemsep{0.5em}
\item Real part: $Re(z) = a$,
\item Imaginary part: $Im(z) = b$,
\item Complex conjugate: $\overline{z}= a -ib$, 
\item Modulus: $|z| = \sqrt{Re(z)^2 + Im(z)^2} = \sqrt{a^2 + b^2}$
\end{itemize}

\vspace{1em}

We have $|z|^2 = z \overline{z}$ and $Re(z) = \frac{z + \overline{z}}{2}$.
\end{frame}


\begin{frame}
\begin{definition}
Let $V$ be an $\F$-vector space. A function $\innerprod{\cdot,\cdot} \colon V \times V \to \F$ is called \emph{inner product} on $V$ if the following holds:
\begin{enumerate}
    \item (Conjugate) symmetry: $\innerprod{\bx,\by} = \overline{\innerprod{\by,\bx}}$ for all $\bx,\by\in V$, where $\overline{a}$ denotes the complex conjugate for $a\in \C$
    \item Linearity in the first argument: $\innerprod{\alpha \bx + \beta \by, \bz} = \alpha \innerprod{\bx,\bz} + \beta \innerprod{\by,\bz}$ for all $\bx,\by,\bz\in V$ and $\alpha, \beta \in \F$
    \item Positive definiteness: $\innerprod{\bx,\bx} \geq 0$ and $\innerprod{\bx,\bx} = 0$ if and only if $\bx = \zerovec$ 
\end{enumerate}
A vector space equipped with an inner product is called an \emph{inner product space}.
\end{definition}

\end{frame}


\begin{frame}
\begin{example}
\begin{itemize}
      \setlength\itemsep{1em}
    \item Standard inner product on $\R^n$: $\innerprod{\bx,\by }= \sum_{i=1}^n x_iy_i$ for $\bx,\by\in \R^n$
    \item Standard inner product on $\C^n$: $\innerprod{\bx,\by }= \sum_{i=1}^n x_i\overline{y}_i$ for $\bx,\by\in \C^n$
    \item On the space of polynomials $\mathbb{P}_n(\R)$: $\innerprod{\boldsymbol{p},\boldsymbol{q}} = \int_{-1}^1 p(x) {q}(x) \mathrm{d}x$ for $\boldsymbol{p},\boldsymbol{q}\in \mathbb{P}_n(\R)$
\end{itemize}
\end{example}

\end{frame}


\begin{frame}
\begin{exampleblock}{Proposition}
Let $V$ be an inner product space. Then $\bx = \zerovec$ if and only if $\innerprod{\bx,\by } = 0$ for all $\by \in V$.
\end{exampleblock}

\begin{proof}
\vspace{4cm}
\end{proof}
\end{frame}


\begin{frame}{Cauchy-Schwarz Inequality}
\begin{exampleblock}{Proposition}
Let $V$ be an inner product space. Then 
\begin{align*}
    \vert \innerprod{\bx,\by}\vert \leq \sqrt{\innerprod{\bx,\bx}}\sqrt{\innerprod{\by,\by}}
\end{align*}
for all $\bx,\by\in V$.
\end{exampleblock}
\end{frame}

\begin{frame}
\begin{proof}
\vspace{6cm}
\end{proof}
\end{frame}


\begin{frame}
\begin{exampleblock}{Proposition}
Let $V$ be an inner product space. Then $\innerprod{\cdot,\cdot}$ induces a norm on $V$ via $\Vert \bx\Vert =\sqrt{\innerprod{\bx,\bx}} $ for all $\bx \in V$.
\end{exampleblock}

\begin{block}{Proof}
\vspace{4cm}
\end{block}
\end{frame}

\begin{frame}
\begin{block}{Proof continued}
\vspace{4cm}
\end{block}
\vspace{1.5cm}
\end{frame}


\begin{frame}{Adjoint}
\begin{definition}
Let $U,V$ be inner product spaces and $S\colon U \to V$ be a linear map. The \emph{adjoint} $S^*$ of $S$ is the linear map $S^*\colon V \to U$ defined such that 
\begin{align*}
    \innerprod{S\bu,\bv}_V = \innerprod{\bu,S^*\bv}_U \qquad \text{ for all } \bu\in U, \bv\in V.
\end{align*}
\end{definition}
\end{frame}


\begin{frame}
\begin{exampleblock}{Proposition}
Let $U,V$ be inner product spaces and $S\colon U \to V$ be a linear map. Then $S^*$ is unique and linear.
\end{exampleblock}

\begin{block}{Proof}
\vspace{4cm}
\end{block}
\end{frame}

\begin{frame}
\begin{block}{Proof continued}
\vspace{6cm}
\end{block}
\end{frame}



\begin{frame}
\begin{example}
Define $S \colon \R^3 \to \R^2$ by $S\bx = (2x_1+x_3,-x_2)$. Then, for all $\by= (y_1,y_2) \in \R^2$ the defining equation for the adjoint operator leads to

\vspace{5cm}

\end{example}
\end{frame}


\begin{frame}
\begin{exampleblock}{Proposition}
Let $A\in M_{m\times n}(\F)$ be a matrix and $T_A\colon \F^n\to F^m \colon \bx \mapsto A\bx$. Then, $T_A^*(\bx) = A^* \bx $, where $A^*\in M_{n\times m}(\F)$ with $(A^*)_{ij} = \overline{A_{ji}}$ for $i=1,\ldots,n$ and $j=1,\ldots,m$. 

\vspace{1em}
In particular, if $\F = \R$, the adjoint of the matrix is given by its transpose,denoted $A^T$, and if $\F = \C$, it is given by its conjugate transpose, denoted $A^*$.
\end{exampleblock}
\end{frame}

\begin{frame}
\begin{proof}
\vspace{6cm}
\end{proof}
\end{frame}


\begin{frame}
\begin{definition}
A matrix $O\in M_n(\R)$ is called \emph{orthogonal} if its inverse is given by its transpose, i.e. $O^T O = O O^T = I$. 

\vspace{0.5em}
A matrix $U\in M_n(\C)$ is called \emph{unitary} if the inverse is given by the conjugate transpose, i.e. $U^* U = U U^* = I$.
\end{definition}
\end{frame}


\begin{frame}
\begin{example}
\begin{itemize}
    \item Let $\varphi \in [0,2\pi]$. Then 
    \begin{equation*}
        \begin{pmatrix}
            \cos(\varphi) & -\sin(\varphi) \\
            \sin(\varphi) & \cos(\varphi)
        \end{pmatrix}
    \end{equation*}
    is an orthogonal matrix. What does it describe geometrically?
    \item The following is a unitary matrix:
        \begin{equation*}
        \begin{pmatrix}
            0 & -i \\
            i & 0
        \end{pmatrix}
    \end{equation*}
\end{itemize}
\end{example}
\end{frame}


\begin{frame}
\begin{definition}
Let $A\in M_n(\F)$. We call $A$ \emph{self-adjoint} if $A^* = A$. In the case $\F = \R$, such an $A$ is called \emph{symmetric} and if $\F = \C$, such an $A$ is called \emph{Hermitian}.
\end{definition}
\end{frame}


\begin{frame}{Orthogonality and Gram-Schmidt}
\begin{definition}
Two vectors $\bx,\by \in V$ are called \emph{orthogonal} if $\innerprod{\bx,\by} = 0$, denoted $\bx \perp \by$. We call them \emph{orthonormal} if additionally the vectors are normalized, i.e. $\Vert \bx \Vert =\Vert \by\Vert = 1 $. A basis $\bx_1,\ldots, \bx_n$ of $V$ is called \emph{orthonormal basis (ONB)}, if the vectors are pairwise orthogonal and normalized.  
\end{definition}

\end{frame}

\begin{frame}
\begin{exampleblock}{Proposition}
Let $\bx_1,\ldots, \bx_k\in V$ be orthonormal. Then the system of vectors is linearly independent.
\end{exampleblock}

\begin{proof}
\vspace{4cm}
\end{proof}
\end{frame}


\begin{frame}
\begin{exampleblock}{Proposition (Orthogonal Decomposition)}
Let $\bx,\by\in V$ with $\by \neq 0$. Then, there exist $c\in F$ and $\bz\in V$ such that $\bx = c\by + \bz$ with $\by \perp \bz$.
\end{exampleblock}

Given a basis we can obtain an ONB from it using the Gram-Schmidt algorithm by reiterating the orthogonal decomposition from above.
\end{frame}

\begin{frame}
\begin{exampleblock}{Proposition (Gram-Schmidt Algorithm)}
Let $\bx_1,\ldots, \bx_n \in V$ be a system of linearly independent vectors. Define $\by_1 = \bx_1/\Vert \bx_1 \Vert$. For $i = 2,\ldots,n$ define $\by_j$ inductively by
\begin{align*}
    \by_i = \frac{\bx_i-\sum_{k=1}^{i-1} \innerprod{\bx_i,\by_k}\by_k}{\Vert \bx_i-\sum_{k=1}^{i-1} \innerprod{\bx_i,\by_k}\by_k\Vert}.
\end{align*}
Then the $\by_1, \ldots, \by_n$ are orthonormal and 
\begin{align*}
    \mathrm{span}\{\bx_1,\ldots,\bx_n\} = \mathrm{span}\{\by_1,\ldots,\by_n\}.
\end{align*}
\end{exampleblock}

The proof is omitted but can be found in the book.
\end{frame}



\begin{frame}{Next time}
    \begin{itemize}
      \setlength\itemsep{0.5em}
	\item Eigenvalues and eigenvectors
	\item Matrix decompositions
    \end{itemize}
\end{frame}




\begin{frame}{References}

Axler S. \textit{Linear Algebra Done Right}. 3rd ed. Undergraduate Texts in Mathematics. Springer, 2015.
Available from: \href{https://link.springer.com/book/10.1007/978-3-319-11080-6}{https://link.springer.com/book/10.1007/978-3-319-11080-6} 
%U of T login:  \href{https://link-springer-com.myaccess.library.utoronto.ca/book/10.1007/978-3-319-11080-6}{https://link-springer-com.myaccess.library.utoronto.ca/book/10.1007/978-3-319-11080-6}

\vspace{1em}


\indent Treil S. \textit{Linear Algebra Done Wrong}. 2017. Available from: \href{https://www.math.brown.edu/streil/papers/LADW/LADW.html}{https://www.math.brown.edu/streil/papers/LADW/LADW.html}
\end{frame}




\end{document}