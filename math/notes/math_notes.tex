\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{setspace}
\pagenumbering{arabic}
\usepackage{graphicx}
\usepackage[dvipsnames]{xcolor}
\usepackage{fancyhdr} 
\usepackage{amsmath, amsfonts, amsthm, amssymb}
\usepackage{bbm}
\usepackage{nth}
\usepackage{dsfont}



\input{commands}

\hypersetup{
  colorlinks   = true, %Colours links instead of boxes
  urlcolor     = black, %Colour for external hyperlinks
  linkcolor    = black, %Colour of internal links
  citecolor   = black %Colour of citations
}

\allowdisplaybreaks % fixes align environment weird spacing on page
\setlength{\parindent}{0cm}


% \usepackage[natbib=true, style=vancouver]{biblatex}
 \usepackage[backend= biber, style=authoryear]{biblatex}
\bibliography{references.bib}

\title{Mathematics Bootcamp \\
\vspace{0.5em}
\large Department of Statistical Sciences, University of Toronto}
\author{Emma Kroell}
\date{Last updated: \today}

\begin{document}

\maketitle
\tableofcontents

\newpage
\section*{Preface}
\addcontentsline{toc}{section}{Preface}
These notes were prepared for the inaugaral Department of Statistical Sciences Graduate Student Bootcamp at the University of Toronto, which is to be held in July 2022. 

References are provided for each section. All references are freely available online, though some may require a University of Toronto library log-in to access. 

\newpage
\section{Review of proof techniques with examples from algebra and analysis}
\subsection{Propositional logic}

{\bf Propositions} are statements that could be true or false. They have a corresponding {\bf truth value}.We will use capital letters to denote propositions. 

\vspace{1em}

ex. ``$n$ is odd'' and ``$n$ is divisible by 2'' are propositions . 

Let's call them $P$ and $Q$. Whether they are true or not (i.e. their truth value) depends on what $n$ is. 

\vspace{1em}

We can  negate statements: $\neg P$ is the statement ``$n$ is not odd''

\vspace{1em}
 We can combine statements: 
 \begin{itemize}
 \item $P \wedge Q$ is the statement ``$n$ is odd and $n$ is divisible by 2''.
 \item $P \vee Q$ is the statement ``$n$ is odd or $n$ is divisible by 2''. We always assume the inclusive or unless specifically stated otherwise.
\end{itemize}

Examples:
\begin{itemize}
              \item If it's not raining, I won't bring my umbrella.
              \item I'm a banana or Toronto is in Canada.
              \item If I pass this exam, I'll be both happy and surprised.
\end{itemize}


\subsubsection{Truth values}

\begin{example} Write the following using propositional logic
If it is snowing, then it is cold out. \\
It is snowing. \\
Therefore, it is cold out.  
\end{example}

\begin{solution}
$P \implies Q$ \\
$P$ \\
Conclusion: $Q$ \\
\end{solution}


To examine if statement is true or not, we use a truth table


\begin{tabular}{|c|c| c|}
\hline
     $P$& $Q$ &  $P \implies Q$ \\ \hline
     T& T & T \\ \hline
     T & F & F \\ \hline
     F & T & T \\ \hline
     F & F & T \\ \hline
\end{tabular}



\subsubsection{Logical equivalence}

\begin{tabular}{|c|c| c|}
\hline
     $P$& $Q$ &  $P \implies Q$ \\ \hline
     T& T & T \\ \hline
     T & F & F \\ \hline
     F & T & T \\ \hline
     F & F & T \\ \hline
\end{tabular} \hspace{2cm} \begin{tabular}{|c | c | c | c|}
\hline
     $P$& $Q$ & $\neg P$ & $\neg P \vee Q$  \\ \hline
     T& T & F & T \\ \hline
     T & F & F & F \\ \hline
     F & T &  T &T \\ \hline
     F & F & T & T \\ \hline
\end{tabular}


What is $\neg (P \implies Q)$?



\subsection{Types of proof}
\begin{itemize}
	\item Direct
	\item Contradiction
	\item Contrapositive
	\item Induction
\end{itemize}


\subsubsection{Direct Proof}

{\bf Approach:} Use the definition and known results.
\vspace{1em}

\begin{example}
The product of an even number with another integer is even.
\end{example}

Approach: use the definition of even.

\begin{definition}
We say that an integer $n$ is {\bf even} if there exists another integer $j$ such that $n=2j$. \\
We say that an integer $n$ is {\bf odd} if there exists another integer $j$ such that $n=2j+1$.
\end{definition}

\begin{proof}
Let $n, m \in \Z$, with $n$ even. By definition, there $\exists$ $j \in \Z$ such that $n = 2j$. Then 
$$ n m  =  (2 j) m = 2 (j m)$$
Therefore $n m$ is even by definition. 
\end{proof}

\subsubsection{Proof by contrapositive}
\begin{example}
If an integer squared is even, then the integer is itself even.
\end{example}


How would you approach this proof?


$P \implies Q$  \hspace{5cm}  $\neg P \implies \neg Q$

        \vspace{1.5em}
\begin{tabular}{|c|c| c|}
\hline
     $P$& $Q$ &  $P \implies Q$ \\ \hline
     T& T & T \\ \hline
     T & F & F \\ \hline
     F & T & T \\ \hline
     F & F & T \\ \hline
\end{tabular}   \hspace{2cm}  \begin{tabular}{|c | c | c |  c | c |}
\hline
     $P$& $Q$ & $\neg P$ &  $\neg Q$ & $\neg Q \implies \neg P$ \\ \hline
     T& T & F & F & T \\ \hline
     T & F & F &  T & T \\ \hline
     F & T &  T  & F & F \\ \hline
     F & F & T & T & T \\ \hline
\end{tabular}
\vspace{1.5em}


\begin{proof}
We prove the contrapositive. Let $n$ be odd. Then there exists $k \in \Z$ such that $n = 2k + 1$. We compute
$$n^2 = (2k + 1)^2 = 4k^2 + 4k + 1 = 2(2k^2+2k) + 1.$$
Thus $n^2$ is odd.

\end{proof}


\subsubsection{Proof by contradiction}
\begin{example}
The sum of a rational number and an irrational number is irrational.
\end{example}

\begin{proof}
Let $q \in \mathbb{Q}$ and $r \in \mathbb{R} \setminus \mathbb{Q}$.
Suppose in order to derive a contradiction that their sum is rational, i.e. $ r + q = s$ where $s \in \mathbb{Q}$.
But then $r = s - q \in \mathbb{Q}$. Contradiction.
\end{proof}




\subsubsection{Summary}

{\bf In sum, to prove $P \implies Q$:} \\

\vspace{1em}


\begin{tabular}{r l}
     Direct proof:  & assume $P$, prove $Q$ \\
     Proof by contrapositive:  & assume $\neg Q$, prove $\neg P$ \\ 
     Proof by contradiction: & assume $P \wedge \neg Q$ and derive something that is impossible \\ 
\end{tabular}


\subsubsection{Induction}

\begin{theorem}[Well-ordering principle for $\mathbb{N}$]
Every nonempty set of natural numbers has a least element.
\end{theorem}

\begin{theorem}[Principle of mathematical induction]
Let $n_0$ be a non-negative integer. Suppose $P$ is a property such that 
\begin{enumerate}
\item(base case) $P(n_0)$ is true 
\item (induction step) For every integer $k \geq n_0$, if $P(k)$ is true, then $P(k+1)$ is true.
\end{enumerate}
Then $P(n)$ is true for every integer $n \geq n_0$
\end{theorem}

Note: Principle of strong mathematical induction: For every integer $k \geq n_0$, if $P(n)$ is true for every $n = n_0, \ldots, k$, then $P(k+1)$ is true.


\begin{example}
$n! > 2^n$ if $n \geq 4$.
\end{example}


\begin{proof}
We prove this by induction on $n$. \\
{\it Base case:} Let $n = 4$. Then $n! = 4! = 24 > 16 = 2^4$. \\
{\it Inductive hypothesis:} Suppose for some $k \geq 4$, $k! > 2^k$. \\
Then
$$(k+1)! = (k+1) k! > (k+1) 2^k > 2 (2^k) = 2^{k+1}.$$
\end{proof}

\begin{example}
Every integer $n \geq 2$ can be written as the product of primes.
\end{example}

\begin{proof}
We prove this by induction on $n$. \\

{\it Base case:} $n = 2$ is prime. \\

{\it Inductive hypothesis:} Suppose for some $k \geq 2$ that one can write every integer $n$ such that $2 \leq n \leq k$ as a product of primes. \\

We must show that we can write $k+1$ as a product of primes. \\
First, if $k+1$ is prime then we are done.  \\

Otherwise, if $k+1$ is not prime, by definition it can be written as a product of some integers $a$, $b$ such that $1 < a,b < k+1$. 
By the induction hypothesis, $a$ and $b$ can both be written as products of primes, so we are done.
\end{proof}


\subsection{Exercises}
\begin{enumerate}
\item Prove De Morgan's Laws: $\neg (P \wedge Q) = \neg P \vee \neg Q$ and $\neg (P \vee Q) = \neg P \wedge \neg Q$ .
\item Prove the Fundamental Theorem of Arithmetic, that every integer $n \geq 2$ has a unique prime factorization (i.e. prove that the prime factorization from the last proof is unique).
\end{enumerate}

\subsubsection{Axioms of a field}
\begin{enumerate}
\setlength\itemsep{0.1em}
    \item[(A1)] \textit{Commutativity in addition:} $x + y = y + x$
    \item[(A2)] \textit{Commutativity in multiplication:} $x \times y = y \times x$
    \item[(B1)] \textit{Associativity in addition:} $x + (y + z) = (x + y) + z$ 
    \item[(B2)] \textit{Associativity in multiplication:} $x \times (y\times z) = (x\times y) \times z$ 
    \item[(C)] \textit{Distributivity:} $x \times (y + z) = x \times y + x \times z$
    \item[(D1)]\textit{Existence of a neutral element, addition:} There exists a number 0 such that $x + 0 = x$ for every $x$.
    \item[(D2)] \textit{Existence of a neutral element, multiplication:} There exists a number 1 such that $x \times 1 = x$ for every $x$. 
    \item[(E1)]\textit{Existence of an inverse, addition:} For each number $x$, there exists a number $-x$ such that $x + (-x) = 0$.
     \item[(E2)]\textit{Existence of an inverse, multiplication:} For each number $x \neq 0$, there exists a number $1/x$ such that $x \times 1/x = 1$.
\end{enumerate}

%\vspace{1em}
%
%We also make the following assumptions about the order of the real numbers:
%\begin{enumerate}
%\setlength\itemsep{0.1em}
%    \item[(F)] \textit{Uniqueness of ordering:} For any $x,y \in \R$, only one of the following holds: $x <y$, $x=y$, or $x > y$.
%    \item[(G)] \textit{Transitivity:} If $x <y$ and $y < z$, then $x < z$.
%    \item[(H1)] \textit{Ordering with addition:} For any $x$, if $y < z$ then $x + y < x + z$.
%    \item[(H2)] \textit{Ordering with multiplication:} For any $x>0$, if $y < z$ then $x \times y < x \times z$.
%\end{enumerate}


\emma{This section to be worked on later}


%\vspace{1em}
%
%We also make the following assumptions about the order of the real numbers:
%\begin{enumerate}
%\setlength\itemsep{0.1em}
%    \item[(F)] \textit{Uniqueness of ordering:} For any $x,y \in \R$, only one of the following holds: $x <y$, $x=y$, or $x > y$.
%    \item[(G)] \textit{Transitivity:} If $x <y$ and $y < z$, then $x < z$.
%    \item[(H1)] \textit{Ordering with addition:} For any $x$, if $y < z$ then $x + y < x + z$.
%    \item[(H2)] \textit{Ordering with multiplication:} For any $x>0$, if $y < z$ then $x \times y < x \times z$.
%\end{enumerate}


\subsection{References}
A good resource for this is \textcite{proofs}. \textcite{toolsreasoning} is also a great resource, but sadly it is not freely available online or at U of T.




\section{Linear Algebra}

\subsection{Vector spaces}
\subsubsection{Axioms of a vector space}
Let $V$ be a set and let $\mathbb{F}$ be a field.

\begin{definition}
\label{def:vec_space}
We call $V$ a \textbf{vector space} if the following hold: \\
Addition:
\begin{enumerate}
\setlength\itemsep{0.1em}
    \item[(A)] \textit{Commutativity in addition:} $\bu + \bv = \bv + \bu$ for all $\bu, \bv \in V$
    \item[(B)] \textit{Associativity in addition:} $\bu + (\bv + \bw) = (\bu + \bv) + \bw$ for all $\bu, \bv, \bw \in V$
    \item[(C)] \textit{Existence of a neutral element, addition:} There exists a vector $\zerovec$ such that for any $\bv \in V$, $\zerovec + \bv = \bv$
    \item[(D)] \textit{Additive inverse:} For every $\bv \in V$, there exists another vector, which we denote $-\bv$, such that $\bv + (-\bv) = \zerovec$.
\end{enumerate}

Multiplication by a scalar:

\begin{enumerate}
\setlength\itemsep{0.1em}
    \item[(E)] \textit{Existence of a neutral element, multiplication:} For any $\bv \in V$, $1\times \bv = \bv$
    \item[(F)] \textit{Associativity in multiplication:} Let $\alpha, \beta \in \mathbb{F}$. For any $\bv \in V$, $(\alpha \beta) \bv = \alpha (\beta \bv)$ 
\end{enumerate}

Associativity:
\begin{enumerate}
\setlength\itemsep{0.1em}
    \item[(G)] Let $\alpha \in \mathbb{F}, \bu, \bv \in V$. $\alpha (\bu + \bv) = \alpha \bu + \beta \bv$.
    \item[(H)] Let $\alpha, \beta \in \mathbb{F}, \bv \in V$. $(\alpha + \beta) \bv = \alpha \bv + \beta \bv$.
\end{enumerate}
\end{definition}

Elements of the vector space are called vectors.

Most often we will assume $\mathbb{F} = \mathbb{C}$ or $\R$.

Examples of vector spaces: $\R^n$. $\mathbb{C}^n$, $M_{m \times n}$ (matrices of size $m \times n$), $\mathbb{P}_n$ (polynomials of degree $n$, $p(x) = a_0 + a_1 x + \ldots + a_n x^n$).

\begin{lemma}
\label{lem:neg_vec}
For every $\bv \in V$, we have $-\bv = (-1) \times \bv$.
\end{lemma}
\begin{proof}
Our goal is to show that $(-1) \times \bv$ is the additive inverse of $\bv$.
We show this as follows:
\begin{align*}
    \bv + (-1) \times \bv = \bv \times (1 + (-1)) = \bv \times 0 = 0
\end{align*}
The last step uses \cref{ex:zero}.
\emma{Do by hand in class}
\end{proof}

\subsubsection{Subspaces}

\begin{definition}
A subset $U$ of $V$ is called a \textbf{subspace} of of $V$ if $U$ is also a vector space (using the same addition and scalar multiplication as on $V$).
\end{definition}


\begin{proposition}\label{prop: characterization of subspace}
A subset $U$ of $V$ is a subspace of $V$ if
and only if $U$ satisfies the following three conditions:
\begin{enumerate}
\item  $\zerovec \in U$
\item Closed under addition: $u,w\in U$ implies $\bu+\bv \in U$
\item Closed under scalar multiplication: $\alpha \in\F$ and $u\in U$
implies $\alpha \bu \in U$
\end{enumerate}
\end{proposition}

\begin{proof}
$\Rightarrow$ If $U$ is a subspace of $V$, then $U$ satisfies these 3 properties by \cref{def:vec_space}.

$\Leftarrow$ Suppose $U$ satisfies the given 3 conditions. Then for any $\bv \in U$, there must exist $-\bv \in U$ by property 3, since $-\bv = (-1) \times \bv$ by \cref{lem:neg_vec} (property D). Property 1 assures property C. Properties 2 and 3, and the fact that $U \subset V$, assure the remaining properties hold. 

\end{proof}

This characterisation allows us to easily show that the intersection of subspaces is again a subspace.

\begin{proposition}
 Let $V$ be a vector space and let $U_1, U_2 \subseteq V$ be subspaces. Then $U_1 \cap U_2$ is also a subspace of $V$. 
\end{proposition}

\begin{proof}
We use the characterization in \cref{prop: characterization of subspace}. First, since $\zerovec \in U_1$ and $\zerovec\in U_2$, we have $\zerovec\in U_1 \cap U_2$. Second, for $\bu, \bv \in U_1\cap U_2$, since in particular $\bu,\bv \in U_1$ and $\bu,\bv \in U_2$ and $U_1, U_2$ are subspaces, $\bu+\bv\in U_1$ and $\bu+\bv\in U_2$. Thus, $\bu+\bv \in U_1 \cap U_2$. Similarly, one shows $\alpha \bu \in U_1\cap U_2$ for $\alpha \in \F$.
\end{proof}

On the contrary the union of two subspaces is not a subspace in general (see \cref{ex: union is not subspace}). However, the next definition introduces the smallest subspace containing the union.

\begin{definition}
Suppose $U_{1},...,U_{m}$ are subsets of $V$. The sum
of $U_{1},...,U_{m}$, denoted $U_{1}+...+U_{m}$, is the set of all
possible sums of elements of $U_{1},...,U_{m}.$ More precisely,
\[
U_{1}+...+U_{m}=\{\bu_{1}+...+\bu_{m}:\bu_{1}\in U_{1},...,\bu_{m}\in U_{m}\}
\]
\end{definition}


\begin{proposition}
Suppose $U_{1},...,U_{m}$ are subspaces of $V$. Then
$U_{1}+...+U_{m}$ is the smallest subspace of $V$ containing $U_{1},...,U_{m}$.
\end{proposition}



\subsubsection{Exercises}
\begin{exercise}[1.1.7 in \cite{linalgwrong}]
\label{ex:zero}
Show that $0 \bv= \zerovec$ for $\bv\in V$.
\end{exercise}
\begin{exercise}[1.B.1 in \cite{linalgright}]
Show that $-(-v)=v$ for $\bv\in V$.
\end{exercise}
\begin{exercise}[1.B.2 in \cite{linalgright}]
Suppose that $\alpha\in\F, \bv\in V$, and $\alpha \bv=0$. Prove that $a=0$
or $v=0$.
\end{exercise}
\begin{exercise}[1.B.4 in \cite{linalgright}]
Why is the empty space not a vector space?
\end{exercise}
\begin{exercise}[7.4.1 in \cite{linalgwrong}]\label{ex: union is not subspace}
Let $U_1$ and $U_2$ be subspaces of a vector space $V$. Prove that $U_1 \cup U_2$ is a subspace of $V$ if and only if $U_1 \subseteq U_2$ or $U_2 \subseteq U_1$.
\end{exercise}


% Exercise:  Give an example of a nonempty subset $U$ of $\mathbb{R}^{2}$ such that $U$ is closed under scalar
% multiplication, but $U$ is not a subspace of $\mathbb{R}$.

% Exercise:  A function $f:\mathbb{R} \rightarrow \mathbb{R}$ is called periodic if there exists a positive number such that $f(x)=f(x+p)$ for 
% all $x\in \mathbb{R}$.  Is the a set of periodic functions from $\mathbb{R}$ to $\mathbb{R}$ a subspace of $\mathbb{R}^{\mathbb{R}}$?

% Exercise:  A function $f:\mathbb{R} \rightarrow \mathbb{R}$ is called odd if
% \[
% f(-x)=-f(x)
% \]

% for all $x\in\mathbb{R}$.  Let $U_{e}$ denote the set of real-valued even functions on $\mathbb{R}$ and let $U_{o}$ denote the set
% of real-valued odd functions on $\mathbb{R}$. Show that $\mathbb{R}^\mathbb{R}=U_{e} \oplus U_{o}$.


\subsection{Linear (in)dependence and bases}

\begin{definition}
A linear combination of vectors $\bv_{1},...,\bv_{n}$  in $V$ is a vector of the form 
$$
\alpha_{1}\bv_{1}+...+\alpha_{n}\bv_{n} = \sum_{k=1}^n \alpha_k \bv_k
$$
 where $\alpha_{1},...,\alpha_{m} \in \F$.
\end{definition}

\begin{definition}
The set of all linear combinations of a list of vectors
$v_{1},...,v_{m}$ in $V$ is called the \textbf{span} of $v_{1},...,v_{m}$,
denoted span$\{\bv_{1},...,\bv_{n}\}$. In other words, 
$$
\text{span}\{\bv_{1},...,\bv_{n}\}=\{\alpha_{1}\bv_{1}+...+\alpha_{m}\bv_{n} :\alpha_{1},...,\alpha_{n}\in\F\}
$$
\end{definition}
The span of the empty list is defined to be $\{\zerovec\}$.

\begin{definition}
A system of vectors $\bv_1, \ldots, \bv_n$ is called a basis (for the vector space $V$ ) if any vector $\bv \in V$ admits a unique representation as a linear combination
$$
\bv = \alpha_1 \bv_1 + \ldots + \alpha_n \bv_n = \sum_{k=1}^n \alpha_k \bv_k
$$
\end{definition}

In undergrad, you likely thought about this as: the equation $\bv = \alpha_1 \bv_1 + \ldots + \alpha_n \bv_n$, where the $x_i$ are unknown, has a unique solution.

Example of bases: \\
For $\R^n$: $e_1 = (1,0,\ldots, 0), \; e_2 = (0,1,0,\ldots,0), \; \ldots, \; e_n = (0, \ldots, 0, 1)$ \\
For $\mathbb{P}^n: \; 1, x, x^2, \ldots, x^n$

\begin{definition}
The linear combination $\alpha_{1}\bv_{1}+...+\alpha_{n}\bv_{n}$ is called trivial if $\alpha_k = 0$ for every $k$.
\end{definition}

\begin{proposition}
 A system of vectors $\bv_1, \ldots \bv_n \in V$ is a basis if and only if it is linearly independent and complete (generating).
\end{proposition}

\emma{Proof done by hand}

\subsubsection{Exercises}
From Harvard:
Exercise: Suppose $v_{1},v_{2},v_{3},v_{4}$ (a) spans $V$ and (b)
is linearly independent. Prove that the list 
\[
v_{1}-v_{2},v_{2}-v_{3},v_{3}-v_{4},v_{4}
\]
 also (a) spans $V$ and (b) is linearly independent. 

\vspace{7mm}

Exercise: Suppose $v_{1},...,v_{m}$ is linearly independent in $V$
and $w\in V$. Prove that if $v_{1}+w,...,v_{m}+w$ is linearly dependent,
then $w\in\text{span}(v_{1},...,v_{m})$. 

Exercise: Suppose that $v_{1},...,v_{m}$ is linearly independent
in $V$ and $w\in V$. Show that $v_{1},...,v_{m},w$ is linearly
independent if and only if 
\[
w\notin\text{span}(v_{1},...,v_{m})
\]
on{Exercises}
Exercise: Suppose $v_{1},v_{2},v_{3},v_{4}$ (a) spans $V$ and (b)
is linearly independent. Prove that the list 
\[
v_{1}-v_{2},v_{2}-v_{3},v_{3}-v_{4},v_{4}
\]
 also (a) spans $V$ and (b) is linearly independent. 

Exercise: Suppose $v_{1},...,v_{m}$ is linearly independent in $V$
and $w\in V$. Prove that if $v_{1}+w,...,v_{m}+w$ is linearly dependent,
then $w\in\text{span}(v_{1},...,v_{m})$. 

Exercise: Suppose that $v_{1},...,v_{m}$ is linearly independent
in $V$ and $w\in V$. Show that $v_{1},...,v_{m},w$ is linearly
independent if and only if 
\[
w\notin\text{span}(v_{1},...,v_{m})
\]

\emma{Add a few from books}

\subsection{Linear transformations}
\begin{definition}
A \textbf{map} $T$ from domain $X$ to codomain $Y$ is a rule that assigns an output $y = T(x) \in Y$ to each input $x \in X$
\end{definition}

\begin{definition}
A map from a vector space $U$ to a vector space $V$ is \textbf{linear} if
\begin{equation*}
    T(\alpha \bu + \beta \bv) = \alpha T(\bu) + \beta T(\bv) \quad \text{for any } \bu, \bv \in V, \; \alpha, \beta \in \F
\end{equation*}
\end{definition}

Let's denote the set of all linear maps from vector space $U$ to vector space $V$ by $\mathcal{L}(U,V)$.

\begin{example}[Differentiation is a linear map]
\label{ex:diff_map}
Let $D \in \mathcal{L}(\mathcal{P}(\R),\mathcal{P}(\R))$, (i.e. $D$ is a linear map from the polynomials on $\R$ to  the polynomials on $\R$), defined as $Dp = p'$. The fact that such a map is linear follows from basic facts about derivatives, i.e. $\frac{d}{dx} (\alpha f(x) + \beta g(x)) = \alpha f'(x) + \beta g'(x)$.

\end{example}

Other examples: integration, rotation of vectors, reflection of vectors

\begin{lemma}
\label{lemm:map_0}
Let $T \in \cL(U,V)$. Then $T(0) = 0$.
\end{lemma}
\begin{proof}
By linearity, $T(0) = T(0+0) = T(0) + T(0)$. Add $-T(0)$ to both sides to obtain the result.
\end{proof}


\begin{theorem}
Let $S,T \in \mathcal{L}(U,V)$ and $\alpha \in \F$. $\mathcal{L}(U,V)$ is a vector space with addition defined as the sum $S+T$ and multiplication as the product $\alpha T$.
\end{theorem}

\begin{definition}[Product of linear maps]
Let $S \in \cL(U,V)$ and $T \in \cL(V,W)$. We define the product $ST \in \cL(U,W)$ for $\bu\in U$ as $ST(\bu) = S(T(\bu))$.
\end{definition}

\begin{definition}
Let $T:U \to V$ be a linear transformation. We define the following important subspaces:
\begin{itemize}
\item \emph{Kernel or null space}: $\ker T = \{\bu \in U : T\bu = 0 \}$
\item \emph{Range} $\range \, T = \{\bv \in V : \exists \bu \in U \text{ such that } \bv = T \bu \}$
\end{itemize}
The dimensions of these spaces are often called the following:
\begin{itemize}
\item \emph{Nullity} $\nullity(T) = \dim(\ker(T))$
\item \emph{Rank} $\rank(T) = \dim(\range(T))$
\end{itemize}
\end{definition}

\begin{example}
The null space of the differentiation map (see \cref{ex:diff_map}) is the set of constant functions.
\end{example}

\begin{definition}[Injective and surjective]
Let $T:U \to V$. $T$ is \emph{injective} if $T\bu = T\bv$ implies $\bu = \bv$ and $T$ is \emph{surjective} if $\forall \bu \in U, \, \exists \bv \in V$ such that $\bv = T\bu$, i.e. if $\range T = V$.
\end{definition}

\begin{theorem}
$T \in \cL(U,v)$ is injective $\Longleftrightarrow$ $\ker T = 0$.
\end{theorem}

\begin{proof}
$\Rightarrow$ Suppose $T$ is injective. By \cref{lemm:map_0}, we know that 0 is in the null space of $T$, i.e. $T(0) = 0$. Suppose $\exists \bv \in \ker T$. Then $T(\bv) = 0 = T(0)$, and by injectivity, $\bv = 0$ . \\
$\Leftarrow$ Suppose $\ker T = 0$. Let $T\bu = T\bv$; we want to show $\bv = \bu$. \\
$T\bu = T\bv$ $\implies$ $T (\bu - \bv) = 0$, which implies $\bu - \bv \in \ker T$. But $\ker T = 0$, so then $\bu - \bv = 0$ $\implies$ $\bu=\bv$.
\end{proof}

\begin{theorem}[Rank Theorem]
For a matrix $A$ or equivalently a linear transformation $A: \F^n \to \F^m$:
\begin{equation*}
\rank A = \rank A^T 
\end{equation*}
\end{theorem}

\begin{theorem}{Rank Nullity Theorem}
Let $T:U \to V$ be a linear transformation, where $U$ and $V$ are finite-dimensional vector spaces. Then  
\begin{equation*}
\rank T + \nullity  T = \dim U.
\end{equation*}
\end{theorem}

\subsubsection{Exercises}
\begin{exercise}
Let $T \in \cL(\mathbb{P}(\R),\mathbb{P}(\R))$ be the map $T (p(x))= x^2 p(x)$ (multiplication by $x^2$). 
\begin{enumerate}
    \item[(i)] Show that $T$ is linear.
    \item[(ii)] Find $\ker T$.
\end{enumerate}
\end{exercise}

\subsection{Linear maps and matrices}


\subsection{Determinants}

\subsection{Inner product spaces}
\emma{transpose, adjoint}

\subsection{Spectral theory}

Note: here we will assume $\F = \C$, so that we are working on an algebraically closed field.


Let $T \colon V \to V$ be a linear map, where $V$ is a vector space.  We would like to describe the action of this linear map in a particularly ``nice'' way. For example, if there exists a basis $\{\bv_1, \ldots, \bv_n\}$ of $V$ such that $T\bv_i = \alpha_i \bv_i$ where $\alpha_i \in \F$ for $i = 1, \ldots, n$, then $T$ acts on this basis merely by scaling the basis vectors. If we look at the matrix of $T$ with respect to this basis, $T$ is a diagonal matrix with $\alpha_i$ in the diagonal. 

\begin{definition}
Let $V$ be a vector space. Given a linear map $T \colon V \to V$ and $\alpha \in \F$, $\alpha$ is called an \textbf{eigenvalue} of $T$ if there exists a non-zero vector $\bv \in V\setminus\{\zerovec\}$ such that $Tv = \alpha v$. We call such $\bv$ an \textbf{eigenvector} of $T$ with eigenvalue $\alpha$. We call the set of all eigenvalues of $T$ \textbf{spectrum} of $T$ and denote it by $\sigma(T)$.
\end{definition}

\emma{Define just for matrices?}

Note that $Tv = \alpha v$ can be rewritten as $(T-\alpha I)v = 0$. Thus, if $\alpha $ is an eigenvalue, the map $T-\alpha I$ is not invertible, since it must have non-trivial kernel. Using the known characterizations of invertability, this gives the following characterization for eigenvalues. 

\begin{theorem}
Let $V$ be a vector space and $T \colon V \to V$ be a linear map and let $A_T$ be a matrix representation of $T$. The following are equivalent
\begin{enumerate}
    \item $\alpha\in \F$ is an eigenvalue of $T$,
    \item $(A_T-\alpha I)\bx = 0$ has a non-trivial solution,
    \item $\det (A_T-\alpha I) = 0$.
\end{enumerate}
\end{theorem}

\begin{theorem}
Suppose $A$ is a square matrix with distinct eigenvalues $\alpha_1, \ldots, \alpha_k$. Let $\bv_1, \ldots, \bv_k$ be eigenvectors corresponding to these eigenvalues. Then $\bv_1, \ldots, \bv_k$ are linearly independent.
\end{theorem}

\begin{proof}
Induction on $k$.
\end{proof}

Hence, if all the eigenvalues are distinct, there exists a basis of eigenvectors. This gives the next result. 

\begin{corollary}
If a $A\in M_n(\C)$ has $n$ distinct eigenvalues, then $A$ is diagonalizable. That is there exists an invertible matrix $U\in M_n(\C)$ such that $A = UDU^\inv$, where $D$ is a diagonal matrix with the eigenvalues of $A$ in the diagonal.
\end{corollary}

\emma{mention problem of eigenspaces not having high enough dimension when eigenvalues are repeated, not necessarily introducing geometric and algebraic multiplicity}

\begin{theorem}
Let $A\in M_n(\C)$ be a Hermitian matrix. Then, there exists a unitary matrix $U\in M_n(\C)$ such that $A=UDU^*$, where $D$ is a diagonal matrix with the eigenvalues of $A$ in the diagonal. Furthermore, all eigenvalues of $A$ are real.
\end{theorem}

\begin{proof}
It suffices to show that there exists an orthogonal basis of eigenvectors and that the eigenvalues are real. We will prove the former by induction.
\end{proof}

Note that in the previous theorem, the orthogonality of the eigenvectors is special. In general, even if a matrix is diagonalizable, there might not exists a orthogonal eigenbasis. The next theorem states a characterization of matrices that exhibit an orthogonal eigenbasis. 

\begin{theorem}
A matrix $A$ is diagonalizable by a unitary matrix if and only if $AA^*= A^*A$. We call such a matrix \textbf{normal}.
\end{theorem}

Proof omitted.


\subsubsection{Exercises}

\begin{exercise}\label{ex: char pol indep of basis}
Let $A,U \in M_n(\F)$ be matrices, where $U$ is invertible. Show that $\sigma(A) =\sigma(UAU^\inv)$.
\end{exercise}

\begin{exercise}
Let $A\in M_n(\C)$ be an invertible matrix with $\sigma(A) = \{\alpha_1,\ldots, \alpha_n\}$ counted with multiplicities. Determine $\sigma(A^\inv)$, $\sigma(A^T)$ and $\sigma(A^*)$.
\end{exercise}


\subsection{Matrix decomposition}


\subsection{References}
The following texts: \\
Linear Algebra Done Right \cite{linalgright} \\
Linear Algebra Done Wrong \cite{linalgwrong}

\section{Set theory}

\section{Metric spaces and sequences}

\section{Topology}

\section{Differentiation and integration}

\section{Multivariable calculus}


\newpage

\printbibliography


\end{document}
