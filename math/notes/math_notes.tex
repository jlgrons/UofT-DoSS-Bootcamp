\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{setspace}
\pagenumbering{arabic}
\usepackage{graphicx}
\usepackage[dvipsnames]{xcolor}
\usepackage{fancyhdr} 
\usepackage{amsmath, amsfonts, amsthm, amssymb}
\usepackage{bbm}
\usepackage{nth}
\usepackage{dsfont}



\input{commands}

\hypersetup{
  colorlinks   = true, %Colours links instead of boxes
  urlcolor     = black, %Colour for external hyperlinks
  linkcolor    = black, %Colour of internal links
  citecolor   = black %Colour of citations
}

\allowdisplaybreaks % fixes align environment weird spacing on page
\setlength{\parindent}{0cm}


%\usepackage[natbib=true, style=vancouver]{biblatex}
 \usepackage[backend= biber, style=authoryear]{biblatex}
\bibliography{references.bib}

\title{Mathematics Bootcamp \\
\vspace{0.5em}
\large Department of Statistical Sciences, University of Toronto}
\author{Emma Kroell}
\date{Last updated: \today}

\begin{document}

\maketitle
\tableofcontents

\newpage
\section*{Preface}
\addcontentsline{toc}{section}{Preface}
These notes were prepared for the inaugaral Department of Statistical Sciences Graduate Student Bootcamp at the University of Toronto, which is to be held in July 2022. 

References are provided for each section. All references are freely available online, though some may require a University of Toronto library log-in to access. 

\newpage
\section{Review of proof techniques with examples from algebra and analysis}
\subsection{Propositional logic}

{\bf Propositions} are statements that could be true or false. They have a corresponding {\bf truth value}.We will use capital letters to denote propositions. 

\vspace{1em}

ex. ``$n$ is odd'' and ``$n$ is divisible by 2'' are propositions . 

Let's call them $P$ and $Q$. Whether they are true or not (i.e. their truth value) depends on what $n$ is. 

\vspace{1em}

We can  negate statements: $\neg P$ is the statement ``$n$ is not odd''

\vspace{1em}
 We can combine statements: 
 \begin{itemize}
 \item $P \wedge Q$ is the statement ``$n$ is odd and $n$ is divisible by 2''.
 \item $P \vee Q$ is the statement ``$n$ is odd or $n$ is divisible by 2''. We always assume the inclusive or unless specifically stated otherwise.
\end{itemize}

Examples:
\begin{itemize}
              \item If it's not raining, I won't bring my umbrella.
              \item I'm a banana or Toronto is in Canada.
              \item If I pass this exam, I'll be both happy and surprised.
\end{itemize}


\subsubsection{Truth values}

\begin{example} Write the following using propositional logic
If it is snowing, then it is cold out. \\
It is snowing. \\
Therefore, it is cold out.  
\end{example}

\begin{solution}
$P \implies Q$ \\
$P$ \\
Conclusion: $Q$ \\
\end{solution}


To examine if statement is true or not, we use a truth table


\begin{tabular}{|c|c| c|}
\hline
     $P$& $Q$ &  $P \implies Q$ \\ \hline
     T& T & T \\ \hline
     T & F & F \\ \hline
     F & T & T \\ \hline
     F & F & T \\ \hline
\end{tabular}



\subsubsection{Logical equivalence}

\begin{tabular}{|c|c| c|}
\hline
     $P$& $Q$ &  $P \implies Q$ \\ \hline
     T& T & T \\ \hline
     T & F & F \\ \hline
     F & T & T \\ \hline
     F & F & T \\ \hline
\end{tabular} \hspace{2cm} \begin{tabular}{|c | c | c | c|}
\hline
     $P$& $Q$ & $\neg P$ & $\neg P \vee Q$  \\ \hline
     T& T & F & T \\ \hline
     T & F & F & F \\ \hline
     F & T &  T &T \\ \hline
     F & F & T & T \\ \hline
\end{tabular}


What is $\neg (P \implies Q)$?



\subsection{Types of proof}
\begin{itemize}
	\item Direct
	\item Contradiction
	\item Contrapositive
	\item Induction
\end{itemize}


\subsubsection{Direct Proof}

{\bf Approach:} Use the definition and known results.
\vspace{1em}

\begin{example}
The product of an even number with another integer is even.
\end{example}

Approach: use the definition of even.

\begin{definition}
We say that an integer $n$ is {\bf even} if there exists another integer $j$ such that $n=2j$. \\
We say that an integer $n$ is {\bf odd} if there exists another integer $j$ such that $n=2j+1$.
\end{definition}

\begin{proof}
Let $n, m \in \Z$, with $n$ even. By definition, there $\exists$ $j \in \Z$ such that $n = 2j$. Then 
$$ n m  =  (2 j) m = 2 (j m)$$
Therefore $n m$ is even by definition. 
\end{proof}

\subsubsection{Proof by contrapositive}
\begin{example}
If an integer squared is even, then the integer is itself even.
\end{example}


How would you approach this proof?


$P \implies Q$  \hspace{5cm}  $\neg P \implies \neg Q$

        \vspace{1.5em}
\begin{tabular}{|c|c| c|}
\hline
     $P$& $Q$ &  $P \implies Q$ \\ \hline
     T& T & T \\ \hline
     T & F & F \\ \hline
     F & T & T \\ \hline
     F & F & T \\ \hline
\end{tabular}   \hspace{2cm}  \begin{tabular}{|c | c | c |  c | c |}
\hline
     $P$& $Q$ & $\neg P$ &  $\neg Q$ & $\neg Q \implies \neg P$ \\ \hline
     T& T & F & F & T \\ \hline
     T & F & F &  T & T \\ \hline
     F & T &  T  & F & F \\ \hline
     F & F & T & T & T \\ \hline
\end{tabular}
\vspace{1.5em}


\begin{proof}
We prove the contrapositive. Let $n$ be odd. Then there exists $k \in \Z$ such that $n = 2k + 1$. We compute
$$n^2 = (2k + 1)^2 = 4k^2 + 4k + 1 = 2(2k^2+2k) + 1.$$
Thus $n^2$ is odd.

\end{proof}


\subsubsection{Proof by contradiction}
\begin{example}
The sum of a rational number and an irrational number is irrational.
\end{example}

\begin{proof}
Let $q \in \mathbb{Q}$ and $r \in \mathbb{R} \setminus \mathbb{Q}$.
Suppose in order to derive a contradiction that their sum is rational, i.e. $ r + q = s$ where $s \in \mathbb{Q}$.
But then $r = s - q \in \mathbb{Q}$. Contradiction.
\end{proof}




\subsubsection{Summary}

{\bf In sum, to prove $P \implies Q$:} \\

\vspace{1em}


\begin{tabular}{r l}
     Direct proof:  & assume $P$, prove $Q$ \\
     Proof by contrapositive:  & assume $\neg Q$, prove $\neg P$ \\ 
     Proof by contradiction: & assume $P \wedge \neg Q$ and derive something that is impossible \\ 
\end{tabular}


\subsubsection{Induction}

\begin{theorem}[Well-ordering principle for $\mathbb{N}$]
Every nonempty set of natural numbers has a least element.
\end{theorem}

\begin{theorem}[Principle of mathematical induction]
Let $n_0$ be a non-negative integer. Suppose $P$ is a property such that 
\begin{enumerate}
\item(base case) $P(n_0)$ is true 
\item (induction step) For every integer $k \geq n_0$, if $P(k)$ is true, then $P(k+1)$ is true.
\end{enumerate}
Then $P(n)$ is true for every integer $n \geq n_0$
\end{theorem}

Note: Principle of strong mathematical induction: For every integer $k \geq n_0$, if $P(n)$ is true for every $n = n_0, \ldots, k$, then $P(k+1)$ is true.


\begin{example}
$n! > 2^n$ if $n \geq 4$.
\end{example}


\begin{proof}
We prove this by induction on $n$. \\
{\it Base case:} Let $n = 4$. Then $n! = 4! = 24 > 16 = 2^4$. \\
{\it Inductive hypothesis:} Suppose for some $k \geq 4$, $k! > 2^k$. \\
Then
$$(k+1)! = (k+1) k! > (k+1) 2^k > 2 (2^k) = 2^{k+1}.$$
\end{proof}

\begin{example}
Every integer $n \geq 2$ can be written as the product of primes.
\end{example}

\begin{proof}
We prove this by induction on $n$. \\

{\it Base case:} $n = 2$ is prime. \\

{\it Inductive hypothesis:} Suppose for some $k \geq 2$ that one can write every integer $n$ such that $2 \leq n \leq k$ as a product of primes. \\

We must show that we can write $k+1$ as a product of primes. \\
First, if $k+1$ is prime then we are done.  \\

Otherwise, if $k+1$ is not prime, by definition it can be written as a product of some integers $a$, $b$ such that $1 < a,b < k+1$. 
By the induction hypothesis, $a$ and $b$ can both be written as products of primes, so we are done.
\end{proof}


\subsection{Exercises}
\begin{enumerate}
\item Prove De Morgan's Laws: $\neg (P \wedge Q) = \neg P \vee \neg Q$ and $\neg (P \vee Q) = \neg P \wedge \neg Q$ .
\item Prove the Fundamental Theorem of Arithmetic, that every integer $n \geq 2$ has a unique prime factorization (i.e. prove that the prime factorization from the last proof is unique).
\end{enumerate}

\subsubsection{Axioms of a field}
\begin{enumerate}
\setlength\itemsep{0.1em}
    \item[(A1)] \textit{Commutativity in addition:} $x + y = y + x$
    \item[(A2)] \textit{Commutativity in multiplication:} $x \times y = y \times x$
    \item[(B1)] \textit{Associativity in addition:} $x + (y + z) = (x + y) + z$ 
    \item[(B2)] \textit{Associativity in multiplication:} $x \times (y\times z) = (x\times y) \times z$ 
    \item[(C)] \textit{Distributivity:} $x \times (y + z) = x \times y + x \times z$
    \item[(D1)]\textit{Existence of a neutral element, addition:} There exists a number 0 such that $x + 0 = x$ for every $x$.
    \item[(D2)] \textit{Existence of a neutral element, multiplication:} There exists a number 1 such that $x \times 1 = x$ for every $x$. 
    \item[(E1)]\textit{Existence of an inverse, addition:} For each number $x$, there exists a number $-x$ such that $x + (-x) = 0$.
     \item[(E2)]\textit{Existence of an inverse, multiplication:} For each number $x \neq 0$, there exists a number $1/x$ such that $x \times 1/x = 1$.
\end{enumerate}

%\vspace{1em}
%
%We also make the following assumptions about the order of the real numbers:
%\begin{enumerate}
%\setlength\itemsep{0.1em}
%    \item[(F)] \textit{Uniqueness of ordering:} For any $x,y \in \R$, only one of the following holds: $x <y$, $x=y$, or $x > y$.
%    \item[(G)] \textit{Transitivity:} If $x <y$ and $y < z$, then $x < z$.
%    \item[(H1)] \textit{Ordering with addition:} For any $x$, if $y < z$ then $x + y < x + z$.
%    \item[(H2)] \textit{Ordering with multiplication:} For any $x>0$, if $y < z$ then $x \times y < x \times z$.
%\end{enumerate}


\emma{This section to be worked on later}


\subsubsection{Exercises}
\begin{enumerate}
\item For any $a,b \neq 0$, $1/(ab) = 1/a \times 1/b$
\item For $a > 0$, $1/(-a) = -1/a$.
\item For $a, b \neq 0$, $1/(a/b) = b/a$
\end{enumerate}
\begin{enumerate}
\setlength\itemsep{0.1em}
    \item[(A1)] \textit{Commutativity in addition:} $x + y = y + x$
    \item[(A2)] \textit{Commutativity in multiplication:} $x \times y = y \times x$
    \item[(B1)] \textit{Associativity in addition:} $x + (y + z) = (x + y) + z$ 
    \item[(B2)] \textit{Associativity in multiplication:} $x \times (y\times z) = (x\times y) \times z$ 
    \item[(C)] \textit{Distributivity:} $x \times (y + z) = x \times y + x \times z$
    \item[(D1)]\textit{Existence of a neutral element, addition:} There exists a number 0 such that $x + 0 = x$ for every $x$.
    \item[(D2)] \textit{Existence of a neutral element, multiplication:} There exists a number 1 such that $x \times 1 = x$ for every $x$. 
    \item[(E1)]\textit{Existence of an inverse, addition:} For each number $x$, there exists a number $-x$ such that $x + (-x) = 0$.
     \item[(E2)]\textit{Existence of an inverse, multiplication:} For each number $x \neq 0$, there exists a number $1/x$ such that $x \times 1/x = 1$.
\end{enumerate}

%\vspace{1em}
%
%We also make the following assumptions about the order of the real numbers:
%\begin{enumerate}
%\setlength\itemsep{0.1em}
%    \item[(F)] \textit{Uniqueness of ordering:} For any $x,y \in \R$, only one of the following holds: $x <y$, $x=y$, or $x > y$.
%    \item[(G)] \textit{Transitivity:} If $x <y$ and $y < z$, then $x < z$.
%    \item[(H1)] \textit{Ordering with addition:} For any $x$, if $y < z$ then $x + y < x + z$.
%    \item[(H2)] \textit{Ordering with multiplication:} For any $x>0$, if $y < z$ then $x \times y < x \times z$.
%\end{enumerate}


\subsection{References}
A good resource for this is \textcite{proofs}. \textcite{toolsreasoning} is also a great resource, but sadly it is not freely available online or at U of T.


\subsubsection{Exercises}
\begin{enumerate}
\item For any $a,b \neq 0$, $1/(ab) = 1/a \times 1/b$
\item For $a > 0$, $1/(-a) = -1/a$.
\item For $a, b \neq 0$, $1/(a/b) = b/a$
\end{enumerate}

\section{Linear Algebra}

\subsection{Vector spaces}
\subsubsection{Axioms of a vector space}
Let $V$ be a set and let $\mathbb{F}$ be a field.

\begin{definition}
\label{def:vec_space}
We call $V$ a \textbf{vector space} if the following hold: \\
Addition:
\begin{enumerate}
\setlength\itemsep{0.1em}
    \item[(A)] \textit{Commutativity in addition:} $\bu + \bv = \bv + \bu$ for all $\bu, \bv \in V$
    \item[(B)] \textit{Associativity in addition:} $\bu + (\bv + \bw) = (\bu + \bv) + \bw$ for all $\bu, \bv, \bw \in V$
    \item[(C)] \textit{Existence of a neutral element, addition:} There exists a vector $\zerovec$ such that for any $\bv \in V$, $\zerovec + \bv = \bv$
    \item[(D)] \textit{Additive inverse:} For every $\bv \in V$, there exists another vector, which we denote $-\bv$, such that $\bv + (-\bv) = \zerovec$.
\end{enumerate}

Multiplication by a scalar:

\begin{enumerate}
\setlength\itemsep{0.1em}
    \item[(E)] \textit{Existence of a neutral element, multiplication:} For any $\bv \in V$, $1\times \bv = \bv$
    \item[(F)] \textit{Associativity in multiplication:} Let $\alpha, \beta \in \mathbb{F}$. For any $\bv \in V$, $(\alpha \beta) \bv = \alpha (\beta \bv)$ 
\end{enumerate}

Associativity:
\begin{enumerate}
\setlength\itemsep{0.1em}
    \item[(G)] Let $\alpha \in \mathbb{F}, \bu, \bv \in V$. $\alpha (\bu + \bv) = \alpha \bu + \beta \bv$.
    \item[(H)] Let $\alpha, \beta \in \mathbb{F}, \bv \in V$. $(\alpha + \beta) \bv = \alpha \bv + \beta \bv$.
\end{enumerate}
\end{definition}

Elements of the vector space are called vectors.

Most often we will assume $\mathbb{F} = \mathbb{C}$ or $\R$.

Examples of vector spaces: $\R^n$. $\mathbb{C}^n$, $M_{m \times n}$ (matrices of size $m \times n$), $\mathbb{P}_n$ (polynomials of degree $n$, $p(x) = a_0 + a_1 x + \ldots + a_n x^n$).

\begin{lemma}
\label{lem:neg_vec}
For every $\bv \in V$, we have $-\bv = (-1) \times \bv$.
\end{lemma}
\begin{proof}
Our goal is to show that $(-1) \times \bv$ is the additive inverse of $\bv$.
We show this as follows:
\begin{align*}
    \bv + (-1) \times \bv = \bv \times (1 + (-1)) = \bv \times 0 = 0
\end{align*}
The last step uses \cref{ex:zero}.
\emma{Do by hand in class}
\end{proof}

\subsubsection{Subspaces}

\begin{definition}
A subset $U$ of $V$ is called a \textbf{subspace} of of $V$ if $U$ is also a vector space (using the same addition and scalar multiplication as on $V$).
\end{definition}


\begin{proposition}\label{prop: characterization of subspace}
A subset $U$ of $V$ is a subspace of $V$ if
and only if $U$ satisfies the following three conditions:
\begin{enumerate}
\item  $\zerovec \in U$
\item Closed under addition: $u,w\in U$ implies $\bu+\bv \in U$
\item Closed under scalar multiplication: $\alpha \in\F$ and $u\in U$
implies $\alpha \bu \in U$
\end{enumerate}
\end{proposition}

\begin{proof}
$\Rightarrow$ If $U$ is a subspace of $V$, then $U$ satisfies these 3 properties by \cref{def:vec_space}.

$\Leftarrow$ Suppose $U$ satisfies the given 3 conditions. Then for any $\bv \in U$, there must exist $-\bv \in U$ by property 3, since $-\bv = (-1) \times \bv$ by \cref{lem:neg_vec} (property D). Property 1 assures property C. Properties 2 and 3, and the fact that $U \subset V$, assure the remaining properties hold. 

\end{proof}

This characterisation allows us to easily show that the intersection of subspaces is again a subspace.

\begin{proposition}
 Let $V$ be a vector space and let $U_1, U_2 \subseteq V$ be subspaces. Then $U_1 \cap U_2$ is also a subspace of $V$. 
\end{proposition}

\begin{proof}
We use the characterization in \cref{prop: characterization of subspace}. First, since $\zerovec \in U_1$ and $\zerovec\in U_2$, we have $\zerovec\in U_1 \cap U_2$. Second, for $\bu, \bv \in U_1\cap U_2$, since in particular $\bu,\bv \in U_1$ and $\bu,\bv \in U_2$ and $U_1, U_2$ are subspaces, $\bu+\bv\in U_1$ and $\bu+\bv\in U_2$. Thus, $\bu+\bv \in U_1 \cap U_2$. Similarly, one shows $\alpha \bu \in U_1\cap U_2$ for $\alpha \in \F$.
\end{proof}

On the contrary the union of two subspaces is not a subspace in general (see \cref{ex: union is not subspace}). However, the next definition introduces the smallest subspace containing the union.

\begin{definition}
Suppose $U_{1},...,U_{m}$ are subsets of $V$. The sum
of $U_{1},...,U_{m}$, denoted $U_{1}+...+U_{m}$, is the set of all
possible sums of elements of $U_{1},...,U_{m}.$ More precisely,
\[
U_{1}+...+U_{m}=\{\bu_{1}+...+\bu_{m}:\bu_{1}\in U_{1},...,\bu_{m}\in U_{m}\}
\]
\end{definition}


\begin{proposition}
Suppose $U_{1},...,U_{m}$ are subspaces of $V$. Then
$U_{1}+...+U_{m}$ is the smallest subspace of $V$ containing $U_{1},...,U_{m}$.
\end{proposition}



\subsubsection{Exercises}
\begin{exercise}[1.1.7 in \cite{linalgwrong}]
\label{ex:zero}
Show that $0 \bv= \zerovec$ for $\bv\in V$.
\end{exercise}
\begin{exercise}[1.B.1 in \cite{linalgright}]
Show that $-(-v)=v$ for $\bv\in V$.
\end{exercise}
\begin{exercise}[1.B.2 in \cite{linalgright}]
Suppose that $\alpha\in\F, \bv\in V$, and $\alpha \bv=0$. Prove that $a=0$
or $v=0$.
\end{exercise}
\begin{exercise}[1.B.4 in \cite{linalgright}]
Why is the empty space not a vector space?
\end{exercise}
\begin{exercise}[7.4.1 in \cite{linalgwrong}]\label{ex: union is not subspace}
Let $U_1$ and $U_2$ be subspaces of a vector space $V$. Prove that $U_1 \cup U_2$ is a subspace of $V$ if and only if $U_1 \subseteq U_2$ or $U_2 \subseteq U_1$.
\end{exercise}


% Exercise:  Give an example of a nonempty subset $U$ of $\mathbb{R}^{2}$ such that $U$ is closed under scalar
% multiplication, but $U$ is not a subspace of $\mathbb{R}$.

% Exercise:  A function $f:\mathbb{R} \rightarrow \mathbb{R}$ is called periodic if there exists a positive number such that $f(x)=f(x+p)$ for 
% all $x\in \mathbb{R}$.  Is the a set of periodic functions from $\mathbb{R}$ to $\mathbb{R}$ a subspace of $\mathbb{R}^{\mathbb{R}}$?

% Exercise:  A function $f:\mathbb{R} \rightarrow \mathbb{R}$ is called odd if
% \[
% f(-x)=-f(x)
% \]

% for all $x\in\mathbb{R}$.  Let $U_{e}$ denote the set of real-valued even functions on $\mathbb{R}$ and let $U_{o}$ denote the set
% of real-valued odd functions on $\mathbb{R}$. Show that $\mathbb{R}^\mathbb{R}=U_{e} \oplus U_{o}$.


\subsection{Linear (in)dependence and bases}

\begin{definition}
A linear combination of vectors $\bv_{1},...,\bv_{n}$ of vectors in $V$ is a vector of the form 
$$
\alpha_{1}\bv_{1}+...+\alpha_{n}\bv_{n} = \sum_{k=1}^n \alpha_k \bv_k
$$
 where $\alpha_{1},...,\alpha_{m} \in \F$.
\end{definition}

\begin{definition}
The set of all linear combinations of a list of vectors
$v_{1},...,v_{m}$ in $V$ is called the \textbf{span} of $v_{1},...,v_{m}$,
denoted span$\{\bv_{1},...,\bv_{n}\}$. In other words, 
$$
\text{span}\{\bv_{1},...,\bv_{n}\}=\{\alpha_{1}\bv_{1}+...+\alpha_{m}\bv_{n} :\alpha_{1},...,\alpha_{n}\in\F\}
$$
\end{definition}
The span of the empty list is defined to be $\{\zerovec\}$.

\begin{definition}
A system of vectors $\bv_1, \ldots, \bv_n$ is called a basis (for the vector space $V$ ) if any vector $\bv \in V$ admits a unique representation as a linear combination
$$
\bv = \alpha_1 \bv_1 + \ldots + \alpha_n \bv_n = \sum_{k=1}^n \alpha_k \bv_k
$$
\end{definition}

In undergrad, you likely thought about this as: the equation $\bv = \alpha_1 \bv_1 + \ldots + \alpha_n \bv_n$, where the $x_i$ are unknown, has a unique solution.

Example of bases: \\
For $\R^n$: $e_1 = (1,0,\ldots, 0), \; e_2 = (0,1,0,\ldots,0), \; \ldots, \; e_n = (0, \ldots, 0, 1)$ \\
For $\mathbb{P}^n: \; 1, x, x^2, \ldots, x^n$

\begin{definition}
The linear combination $\alpha_{1}\bv_{1}+...+\alpha_{n}\bv_{n}$ is called trivial if $\alpha_k = 0$ for every $k$.
\end{definition}

\begin{proposition}
 A system of vectors $\bv_1, \ldots \bv_n \in V$ is a basis if and only if it is linearly independent and complete (generating).
\end{proposition}

\emma{Proof done by hand}

\subsection{Exercises}
From Harvard:
Exercise: Suppose $v_{1},v_{2},v_{3},v_{4}$ (a) spans $V$ and (b)
is linearly independent. Prove that the list 
\[
v_{1}-v_{2},v_{2}-v_{3},v_{3}-v_{4},v_{4}
\]
 also (a) spans $V$ and (b) is linearly independent. 

\vspace{7mm}

Exercise: Suppose $v_{1},...,v_{m}$ is linearly independent in $V$
and $w\in V$. Prove that if $v_{1}+w,...,v_{m}+w$ is linearly dependent,
then $w\in\text{span}(v_{1},...,v_{m})$. 

Exercise: Suppose that $v_{1},...,v_{m}$ is linearly independent
in $V$ and $w\in V$. Show that $v_{1},...,v_{m},w$ is linearly
independent if and only if 
\[
w\notin\text{span}(v_{1},...,v_{m})
\]
on{Exercises}
Exercise: Suppose $v_{1},v_{2},v_{3},v_{4}$ (a) spans $V$ and (b)
is linearly independent. Prove that the list 
\[
v_{1}-v_{2},v_{2}-v_{3},v_{3}-v_{4},v_{4}
\]
 also (a) spans $V$ and (b) is linearly independent. 

Exercise: Suppose $v_{1},...,v_{m}$ is linearly independent in $V$
and $w\in V$. Prove that if $v_{1}+w,...,v_{m}+w$ is linearly dependent,
then $w\in\text{span}(v_{1},...,v_{m})$. 

Exercise: Suppose that $v_{1},...,v_{m}$ is linearly independent
in $V$ and $w\in V$. Show that $v_{1},...,v_{m},w$ is linearly
independent if and only if 
\[
w\notin\text{span}(v_{1},...,v_{m})
\]

\emma{Add a few from books}

\subsection{Linear transformations}
\begin{definition}
A \textbf{transformation} $T$ from domain $X$ to codomain $Y$ is a rule that assigns an output $y = T(x) \in Y$ to each input $x \in X$
\end{definition}

\begin{definition}
A transformation from a vector space $U$ to a vector space $V$ is \textbf{linear} if
\begin{equation*}
    T(\alpha \bu + \beta \bv) = \alpha T(\bu) + \beta T(\bv) \quad \text{for any } \bu, \bv \in V, \; \alpha, \beta \in \F
\end{equation*}
\end{definition}

Examples: differentiation, rotation of vectors, reflection of vectors

\emma{transpose, adjoint}

\subsection{Determinants}

\subsection{Spectral theory}
Let $T \colon V \to V$ be a linear map, where $V$ is a vector space.  We would like to describe the action of this linear map in a particularly nice way. For example, if there exists a basis $\{\bv_1, \ldots, \bv_n\}$ of $V$ such that $T\bv_i = \alpha_i \bv_i$ where $\alpha_i \in \F$ for $i = 1, \ldots, n$, then $T$ acts on this particular basis merely by scaling the basis vectors. If we look at the matrix of $T$ with respect to this basis, $T$ is a diagonal matrix with $\alpha_i$ in the diagonal. 

\begin{definition}
Let $V$ be a vector space. Given a linear map $T \colon V \to V$ and $\alpha \in \F$, $\alpha$ is called an \textbf{eigenvalue} of $T$ if there exists a non-zero vector $\bv \in V\setminus\{\zerovec\}$ such that $Tv = \alpha v$. We call such $\bv$ an \textbf{eigenvector} of $T$ with eigenvalue $\alpha$.
\end{definition}

Note that $Tv = \alpha v$ can be rewritten as $(T-\alpha I)v = 0$. Thus, if $\alpha $ is an eigenvalue, the map $T-\alpha I$ is not invertible, since it must have non-trivial kernel. Using the known characterizations of invertability, this gives the following characterization for eigenvalues.

\begin{theorem}
Let $V$ be a vector space and $T \colon V \to V$ be a linear map. The following are equivalent
\begin{enumerate}
    \item $\alpha\in \F$ is an eigenvalue of $T$,
    \item $(T-\alpha I)\bx = 0$ has a non-trivial solution,
    \item $\det (T-\alpha I) = 0$.
\end{enumerate}
\end{theorem}

Note that the last part is to be understood as the determinant of a matrix description of $T$ with respect to a basis of $V$. We call the polynomial $p_A(\alpha) = \det (T-\alpha I)$ characteristic polynomial of $T$. Hence, the eigenvalues of $T$ are the zeros of the characteristic polynomial. Recall that basis transformations can be expressed as invertible matrices, thus this is well-defined, since the characteristic polynomial and therefore the eigenvalues are independent of the choice of basis.  We will now examine when there exists a basis of eigenvectors. 

\subsection{Inner product spaces}

\subsection{Matrix decomposition}


\subsection{References}
The following texts: \\
Linear Algebra Done Right \cite{linalgright} \\
Linear Algebra Done Wrong \cite{linalgwrong}

\section{Set theory}

\section{Metric spaces and sequences}

\section{Topology}

\section{Differentiation and integration}

\section{Multivariable calculus}


\newpage

\printbibliography


\end{document}
